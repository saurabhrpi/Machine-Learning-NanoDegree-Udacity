{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named environment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1e181bc2ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplanner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoutePlanner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msimulator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named environment"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\" An agent that learns to drive in the Smartcab world.\n",
    "        This is the object you will be modifying. \"\"\" \n",
    "\n",
    "    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):\n",
    "        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment \n",
    "        self.planner = RoutePlanner(self.env, self)  # Create a route planner\n",
    "        self.valid_actions = self.env.valid_actions  # The set of valid actions\n",
    "\n",
    "        # Set parameters of the learning agent\n",
    "        self.learning = learning # Whether the agent is expected to learn\n",
    "        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples\n",
    "        self.epsilon = epsilon   # Random exploration factor\n",
    "        self.alpha = alpha       # Learning factor\n",
    "\n",
    "        ###########\n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # Set any additional class parameters as needed\n",
    "\n",
    "\n",
    "    def reset(self, destination=None, testing=False):\n",
    "        \"\"\" The reset function is called at the beginning of each trial.\n",
    "            'testing' is set to True if testing trials are being used\n",
    "            once training trials have completed. \"\"\"\n",
    "\n",
    "        # Select the destination as the new location to route to\n",
    "        self.planner.route_to(destination)\n",
    "        \n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # Update epsilon using a decay function of your choice\n",
    "        # Update additional class parameters as needed\n",
    "        # If 'testing' is True, set epsilon and alpha to 0\n",
    "\n",
    "        return None\n",
    "\n",
    "    def build_state(self):\n",
    "        \"\"\" The build_state function is called when the agent requests data from the \n",
    "            environment. The next waypoint, the intersection inputs, and the deadline \n",
    "            are all features available to the agent. \"\"\"\n",
    "\n",
    "        # Collect data about the environment\n",
    "        waypoint = self.planner.next_waypoint() # The next waypoint \n",
    "        inputs = self.env.sense(self)           # Visual input - intersection light and traffic\n",
    "        deadline = self.env.get_deadline(self)  # Remaining deadline\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        \n",
    "        # NOTE : you are not allowed to engineer eatures outside of the inputs available.\n",
    "        # Because the aim of this project is to teach Reinforcement Learning, we have placed \n",
    "        # constraints in order for you to learn how to adjust epsilon and alpha, and thus learn about the balance between exploration and exploitation.\n",
    "        # With the hand-engineered features, this learning process gets entirely negated.\n",
    "        \n",
    "        # Set 'state' as a tuple of relevant data for the agent        \n",
    "        state = None\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def get_maxQ(self, state):\n",
    "        \"\"\" The get_max_Q function is called when the agent is asked to find the\n",
    "            maximum Q-value of all actions based on the 'state' the smartcab is in. \"\"\"\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # Calculate the maximum Q-value of all actions for a given state\n",
    "\n",
    "        maxQ = None\n",
    "\n",
    "        return maxQ \n",
    "\n",
    "\n",
    "    def createQ(self, state):\n",
    "        \"\"\" The createQ function is called when a state is generated by the agent. \"\"\"\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # When learning, check if the 'state' is not in the Q-table\n",
    "        # If it is not, create a new dictionary for that state\n",
    "        #   Then, for each action available, set the initial Q-value to 0.0\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\" The choose_action function is called when the agent is asked to choose\n",
    "            which action to take, based on the 'state' the smartcab is in. \"\"\"\n",
    "\n",
    "        # Set the agent state and default action\n",
    "        self.state = state\n",
    "        self.next_waypoint = self.planner.next_waypoint()\n",
    "        action = None\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # When not learning, choose a random action\n",
    "        # When learning, choose a random action with 'epsilon' probability\n",
    "        # Otherwise, choose an action with the highest Q-value for the current state\n",
    "        # Be sure that when choosing an action with highest Q-value that you randomly select between actions that \"tie\".\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward):\n",
    "        \"\"\" The learn function is called after the agent completes an action and\n",
    "            receives a reward. This function does not consider future rewards \n",
    "            when conducting learning. \"\"\"\n",
    "\n",
    "        ########### \n",
    "        ## TO DO ##\n",
    "        ###########\n",
    "        # When learning, implement the value iteration update rule\n",
    "        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" The update function is called when a time step is completed in the \n",
    "            environment for a given trial. This function will build the agent\n",
    "            state, choose an action, receive a reward, and learn if enabled. \"\"\"\n",
    "\n",
    "        state = self.build_state()          # Get current state\n",
    "        self.createQ(state)                 # Create 'state' in Q-table\n",
    "        action = self.choose_action(state)  # Choose an action\n",
    "        reward = self.env.act(self, action) # Receive a reward\n",
    "        self.learn(state, action, reward)   # Q-learn\n",
    "\n",
    "        return\n",
    "        \n",
    "\n",
    "def run():\n",
    "    \"\"\" Driving function for running the simulation. \n",
    "        Press ESC to close the simulation, or [SPACE] to pause the simulation. \"\"\"\n",
    "\n",
    "    ##############\n",
    "    # Create the environment\n",
    "    # Flags:\n",
    "    #   verbose     - set to True to display additional output from the simulation\n",
    "    #   num_dummies - discrete number of dummy agents in the environment, default is 100\n",
    "    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)\n",
    "    env = Environment(num_dummies = 10)\n",
    "    \n",
    "    ##############\n",
    "    # Create the driving agent\n",
    "    # Flags:\n",
    "    #   learning   - set to True to force the driving agent to use Q-learning\n",
    "    #    * epsilon - continuous value for the exploration factor, default is 1\n",
    "    #    * alpha   - continuous value for the learning rate, default is 0.5\n",
    "    agent = env.create_agent(LearningAgent)\n",
    "    \n",
    "    ##############\n",
    "    # Follow the driving agent\n",
    "    # Flags:\n",
    "    #   enforce_deadline - set to True to enforce a deadline metric\n",
    "    env.set_primary_agent(agent)\n",
    "\n",
    "    ##############\n",
    "    # Create the simulation\n",
    "    # Flags:\n",
    "    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds\n",
    "    #   display      - set to False to disable the GUI if PyGame is enabled\n",
    "    #   log_metrics  - set to True to log trial and simulation results to /logs\n",
    "    #   optimized    - set to True to change the default log file name\n",
    "    sim = Simulator(env)\n",
    "    \n",
    "    ##############\n",
    "    # Run the simulator\n",
    "    # Flags:\n",
    "    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 \n",
    "    #   n_test     - discrete number of testing trials to perform, default is 0\n",
    "    sim.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
